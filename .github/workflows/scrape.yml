name: Daily Scrape

on:
  schedule:
    # Run daily at 9 AM UTC (3 AM Central)
    - cron: '0 9 * * *'
  workflow_dispatch:  # Manual trigger button

permissions:
  contents: write  # Need write to commit changes

jobs:
  scrape:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: .
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          cd scraper
          pip install -r requirements.txt
          playwright install chromium

      - name: Run scrapers
        id: scrape
        continue-on-error: true  # Continue even if scrapers fail
        run: |
          cd scraper
          python run_scrape.py 2>&1 | tee scrape_output.txt
          exit_code=$?
          echo "exit_code=$exit_code" >> $GITHUB_OUTPUT
          exit $exit_code

      - name: Copy to public
        run: |
          cp scraper/output/events.json public/events.json
          cp scraper/output/history.json public/history.json

      - name: Commit changes
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add public/events.json public/history.json
          git diff --staged --quiet || git commit -m "Scrape: $(date -u +%Y-%m-%d)"
          git push

      # Create summary with scraper output
      - name: Create job summary
        if: always()
        run: |
          echo "## Scraper Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          cat scraper/scrape_output.txt >> $GITHUB_STEP_SUMMARY || echo "No output captured" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

      # Fail the workflow if scrapers had errors (triggers GitHub notification)
      - name: Check for failures
        if: steps.scrape.outputs.exit_code != '0'
        run: |
          echo "::error::One or more scrapers failed. Check the summary above."
          exit 1
